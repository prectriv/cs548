# Linear Regression

- Linear regression is a simple approach to supervised learning. It assumes the dependence of Y on X1, X2,... Xp is linear.
- True regression functions are never linear
- We are using a function that may be simpler than the actual relationship

## Model

![alt text](img/6/linregmodel.png)

- B0 and B1 are two unknown constants that represent the intercept and slope
  - Known as coefficients or parameters
- e is the error term
- Given some estimates on B0 and B1 for the model coefficients, we predict using:
![prediction](img/6/prediction.png)
- We can estimate by least squares:
  - ![alt text](img/6/estimate.png)
  - ![alt text](img/6/estimate2.png)
- Example:
![example](img/6/linregex.png)

## Assessing Accuracy

- Calculating standard error of an estimator reflects how it varies under repeated sampling
![alt text](img/6/accuracyequ.png)
- These standard errors can be used to compute *confidence intervals*
  - A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value o the parameter. Has the form:
  - ![alt text](img/6/confidenceinterval.png)
- Hypothesis testing
  - H0 (null hypothesis): there is no relationship between X and Y
    - Mathematically, H0: B1 = 0
    - Model reduces to Y = B0 + e so X is not associated with Y
    - To test, we compute the t-statistic:
      - ![alt text](img/6/tstat.png)
  - HA (alternative hypothesis): There is some relationship between X and Y
    - Mathematically, Ha: B1 =/= 0
  - The result of these tests on the example data:
  - ![alt text](img/6/resultsofad.png)

## Assessing Overall Accuracy of the Model

- Residual Standard Error
![alt text](img/6/RSE.png)
- Residual Sum-of-squares
![alt text](img/6/rsos.png)
- R-squared
![alt text](img/6/rsquared.png)
- It can be shown that in the simple linear regression setting, R^2 = r^2 where r is the correlation between X and Y
![alt text](img/6/r.png)
- Advertising data results:
![alt text](img/6/adacc.png)
