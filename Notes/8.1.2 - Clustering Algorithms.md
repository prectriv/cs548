# Clustering Algorithms

## K-means and its variants

- Partitional clustering approach
- Each cluster is associated with a centroid
- Each point is assigned to the cluster with the closest centroid
- Number of clusters, K, must be specified
- The basic algorithm is very simple
- ![alt text](img/8/kmeansalg.png)

### General notes

- Initial centroids are often chosen randomly
  - Clusters produced vary from one run to another
  - Because of this, usually multiple runs are done and comparing each one with the following SSE formula.
- The centroid is typically the mean of the points in the cluster
- 'Closeness' is measured by Euclidean distance, cosine similarity, correlation, etc.
- K-means will not converge for common similarity measures mentioned above
- Most of the convergence happens in the first few iterations
  - Often the stopping condition is changed to 'until relatively few points change clusters'
- Complexity is O(n\*K\*i\*d)
  - n = number of points
  - K = number of clusters
  - i = number of iterations
  - d = number of attributes

### Evaluating its clusters

- Most common measure is Sum of Squared Error (SSE)
  - For each point, the error is the distance of the nearest cluster
  - To get SSE, we square these errors and sum them
  - ![alt text](img/8/sse.png)
    - X is a data point
    - Ci and mi are the representative point for cluster Ci
  - Given two clusters, we choose the one with the smaller SSE value








- Hierarchical clustering
- Density-based clustering